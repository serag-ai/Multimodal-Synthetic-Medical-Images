{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serag-ai/I-SynMed/blob/main/Paper_GrayScale_Conditional_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on\n",
        "https://iopscience.iop.org/article/10.1088/1361-6560/acca5c/meta"
      ],
      "metadata": {
        "id": "YnUUxylk7wzg"
      },
      "id": "YnUUxylk7wzg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Imports"
      ],
      "metadata": {
        "id": "q0V8p8k572-F"
      },
      "id": "q0V8p8k572-F"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/shaoyanpan/2D-Medical-Denoising-Diffusion-Probabilistic-Model-.git"
      ],
      "metadata": {
        "id": "xUWT8vMxBF1I"
      },
      "id": "xUWT8vMxBF1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/2D-Medical-Denoising-Diffusion-Probabilistic-Model-"
      ],
      "metadata": {
        "id": "ah3C0KvCBL1W"
      },
      "id": "ah3C0KvCBL1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install monai\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "gmA0LV5gC1pU"
      },
      "id": "gmA0LV5gC1pU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive if needed,"
      ],
      "metadata": {
        "id": "uoJjC8W678CG"
      },
      "id": "uoJjC8W678CG"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "LFXhVy0IRbqO"
      },
      "id": "LFXhVy0IRbqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import scipy.io\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from scipy import ndimage\n",
        "from skimage import io\n",
        "from skimage import transform\n",
        "from natsort import natsorted\n",
        "from skimage.transform import rotate, AffineTransform\n",
        "from timm.models.layers import DropPath, to_3tuple, trunc_normal_\n",
        "import  torchvision.transforms as transforms\n",
        "#The diffusion module adpated from https://github.com/openai/guided-diffusion\n",
        "from diffusion.Create_diffusion import *\n",
        "from diffusion.resampler import *\n",
        "from diffusion.normal_diffusion import GaussianDiffusionSampler, GaussianDiffusionTrainer\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "from network.Diffusion_model_transformer import *\n",
        "from torchvision.datasets import ImageFolder\n"
      ],
      "metadata": {
        "id": "kkPyMHIe8Rrm"
      },
      "id": "kkPyMHIe8Rrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b39e4aa7",
      "metadata": {
        "id": "b39e4aa7"
      },
      "source": [
        "# Setup Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a750f5c1",
      "metadata": {
        "id": "a750f5c1"
      },
      "outputs": [],
      "source": [
        "# @markdown \\\n",
        "# @markdown # Image Hyperparameters\n",
        "# @markdown \\\n",
        "image_size = 256 # @param {type:\"integer\"}\n",
        "spacing = (1, 1) # @param {type:\"raw\"}\n",
        "channels = 1# @param {type:\"integer\"}\n",
        "\n",
        "# @markdown \\\n",
        "# @markdown # Diffusion Hyperparameters\n",
        "# @markdown \\\n",
        "diffusion_steps = 1000 # @param {type:\"integer\"}\n",
        "learn_sigma=True# @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @markdown \\\n",
        "# @markdown # Network Parameters\n",
        "# @markdown Here enter your network parameters:num_channels means the initial channels in each block, channel_mult means the multipliers of the channels (in this case, 128,128,256,256,512,512 for the first to the sixth block), attention_resulution means we use the transformer blocks in the third to the sixth block number of heads, window size in each transformer block\n",
        "# @markdown \\\n",
        "# @markdown \\\n",
        "num_channels=128 # @param {type:\"integer\"}\n",
        "channel_mult = (1, 1, 2, 2, 4, 4)# @param {type:\"raw\"}\n",
        "attention_resolutions=\"64,32,16,8\" # @param {type:\"string\"}\n",
        "num_heads=[4,4,4,8,16,16]# @param {type:\"raw\"}\n",
        "window_size = [[4,4],[4,4],[4,4],[8,8],[8,8],[4,4]]# @param {type:\"raw\"}\n",
        "num_res_blocks = [2,2,1,1,1,1]# @param {type:\"raw\"}\n",
        "sample_kernel=([2,2],[2,2],[2,2],[2,2],[2,2]),\n",
        "use_scale_shift_norm=True# @param {type:\"boolean\"}\n",
        "resblock_updown = False# @param {type:\"boolean\"}\n",
        "attention_ds = []\n",
        "for res in attention_resolutions.split(\",\"):\n",
        "    attention_ds.append(int(res))\n",
        "\n",
        "\n",
        "# @markdown \\\n",
        "# @markdown # Training Parameters\n",
        "# @markdown \\\n",
        "N_EPOCHS = 1000 # @param {type:\"integer\"}\n",
        "BATCH_SIZE_TRAIN = 4 # @param {type:\"integer\"}\n",
        "class_cond = True# @param {type:\"boolean\"}\n",
        "NUM_CLASSES=4# @param {type:\"integer\"}\n",
        "learning_rate = 5e-6# @param {type:\"number\"}\n",
        "weight_decay =  1e-4# @param {type:\"number\"}\n",
        "training_dt_dir = \"/content/path_to_data\"# @param {type:\"string\"}\n",
        "output_dir = \"/content/tmp/\"# @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't toch these parameters, they are irrelant to the image synthesis\n",
        "sigma_small=False\n",
        "noise_schedule='linear'\n",
        "use_kl=False\n",
        "predict_xstart=False\n",
        "rescale_timesteps=True\n",
        "rescale_learned_sigmas=True\n",
        "use_checkpoint=False\n",
        "img_size = (image_size,image_size)\n",
        "BATCH_SIZE_TRAIN = BATCH_SIZE_TRAIN * channels\n",
        "timestep_respacing=[50]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "76A3iQAy-96g"
      },
      "id": "76A3iQAy-96g",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "65a3f0d8",
      "metadata": {
        "id": "65a3f0d8"
      },
      "source": [
        "# Build the Diffusion process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9698a4ad",
      "metadata": {
        "id": "9698a4ad"
      },
      "outputs": [],
      "source": [
        "diffusion = create_gaussian_diffusion(\n",
        "    steps=diffusion_steps,\n",
        "    learn_sigma=learn_sigma,\n",
        "    sigma_small=sigma_small,\n",
        "    noise_schedule=noise_schedule,\n",
        "    use_kl=use_kl,\n",
        "    predict_xstart=predict_xstart,\n",
        "    rescale_timesteps=rescale_timesteps,\n",
        "    rescale_learned_sigmas=rescale_learned_sigmas,\n",
        "    timestep_respacing=timestep_respacing,\n",
        ")\n",
        "schedule_sampler = UniformSampler(diffusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2e1924",
      "metadata": {
        "id": "ab2e1924"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b6250173",
      "metadata": {
        "id": "b6250173"
      },
      "outputs": [],
      "source": [
        "model = SwinVITModel(\n",
        "        image_size=(image_size,image_size),\n",
        "        in_channels=channels,\n",
        "        model_channels=num_channels,\n",
        "        out_channels=channels*2,\n",
        "        sample_kernel=sample_kernel,\n",
        "        num_res_blocks=num_res_blocks,\n",
        "        attention_resolutions=tuple(attention_ds),\n",
        "        dropout=0,\n",
        "        channel_mult=channel_mult,\n",
        "        num_classes=(NUM_CLASSES if class_cond else None),\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=num_heads,\n",
        "        window_size = window_size,\n",
        "        num_head_channels=64,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=use_scale_shift_norm,\n",
        "        resblock_updown=resblock_updown,\n",
        "        use_new_attention_order=False,\n",
        "    ).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0393fe2b",
      "metadata": {
        "id": "0393fe2b"
      },
      "outputs": [],
      "source": [
        "# #In case you want to use CNN\n",
        "# from network.Diffusion_model_Unet import *\n",
        "# model = UNetModel(\n",
        "#         image_size=image_size,\n",
        "#         in_channels=1,\n",
        "#         model_channels=num_channels,\n",
        "#         out_channels=2,\n",
        "#         num_res_blocks=num_res_blocks[0],\n",
        "#         attention_resolutions=tuple(attention_ds),\n",
        "#         dropout=0.,\n",
        "#         sample_kernel=sample_kernel,\n",
        "#         channel_mult=channel_mult,\n",
        "#         num_classes=(NUM_CLASSES if class_cond else None),\n",
        "#         use_checkpoint=False,\n",
        "#         use_fp16=False,\n",
        "#         num_heads=4,\n",
        "#         num_head_channels=64,\n",
        "#         num_heads_upsample=-1,\n",
        "#         use_scale_shift_norm=use_scale_shift_norm,\n",
        "#         resblock_updown=False,\n",
        "#         use_new_attention_order=False,\n",
        "#     ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d12c7c5",
      "metadata": {
        "id": "2d12c7c5"
      },
      "source": [
        "# Call the optimizer and ready for start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e983d0f",
      "metadata": {
        "id": "4e983d0f"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print('parameter number is '+str(pytorch_total_params))\n",
        "torch.backends.cudnn.benchmark = True\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay = weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb44eb1",
      "metadata": {
        "id": "4bb44eb1"
      },
      "source": [
        "# Build the training function. Run the training function once = one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6283a4a7",
      "metadata": {
        "id": "6283a4a7"
      },
      "outputs": [],
      "source": [
        "# Here we explain the training process\n",
        "def train(model, optimizer,data_loader1, loss_history):\n",
        "\n",
        "    #1: set the model to training mode\n",
        "    model.train()\n",
        "    total_samples = len(data_loader1.dataset)\n",
        "    loss_sum = []\n",
        "    total_time = 0\n",
        "\n",
        "    #2: Loop the whole dataset, x1 (traindata) is the image batch\n",
        "    for i, (x1,labels) in enumerate(data_loader1):\n",
        "\n",
        "        traindata = x1.to(device)\n",
        "        trainlabel = labels.to(device)\n",
        "\n",
        "\n",
        "        #3: extract random timestep for training\n",
        "        t, weights = schedule_sampler.sample(traindata.shape[0], device)\n",
        "\n",
        "        aa = time.time()\n",
        "\n",
        "        #4: Optimize the TDM network\n",
        "        optimizer.zero_grad()\n",
        "        all_loss = diffusion.training_losses(model,traindata,t=t,model_kwargs={'y':trainlabel})\n",
        "        loss = (all_loss[\"loss\"] * weights).mean()\n",
        "        loss.backward()\n",
        "        loss_sum.append(loss.detach().cpu().numpy())\n",
        "        optimizer.step()\n",
        "\n",
        "        #5:print out the intermediate loss for every 100 batches\n",
        "        total_time += time.time()-aa\n",
        "        if i % 100 == 0:\n",
        "            print('optimization time: '+ str(time.time()-aa))\n",
        "            print('[' +  '{:5}'.format(i * BATCH_SIZE_TRAIN) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader1)) + '%)]  Loss: ' +\n",
        "                  '{:6.7f}'.format(np.nanmean(loss_sum)))\n",
        "\n",
        "    #6: print out the average loss for this epoch\n",
        "    average_loss = np.nanmean(loss_sum)\n",
        "    loss_history.append(average_loss)\n",
        "    print(\"Total time per sample is: \"+str(total_time))\n",
        "    print('Averaged loss is: '+ str(average_loss))\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0b32db",
      "metadata": {
        "id": "ae0b32db"
      },
      "source": [
        "# Build the testing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "105082c0",
      "metadata": {
        "id": "105082c0"
      },
      "outputs": [],
      "source": [
        "# Run the evaluate function will generate 4 samples and will be save to a folder in MAT format\n",
        "num_sample = 4 if class_cond ==False else NUM_CLASSES\n",
        "conditions=torch.arange(NUM_CLASSES).long().to(device)\n",
        "\n",
        "def evaluate(model,epoch,path):\n",
        "    model.eval()\n",
        "    aa = time.time()\n",
        "    prediction = []\n",
        "    true = []\n",
        "    img = []\n",
        "    loss_all = []\n",
        "    with torch.no_grad():\n",
        "        x_clean = diffusion.p_sample_loop(model,(num_sample, channels, image_size, image_size),clip_denoised=True,model_kwargs={'y':conditions},)\n",
        "        img.append(x_clean.cpu().numpy())\n",
        "    print('Generate for the epoch #'+str(epoch)+' result:')\n",
        "    plt.rcParams['figure.figsize'] = [20, 20]\n",
        "    plt.figure()\n",
        "    f, axarr = plt.subplots(1,num_sample)\n",
        "    for ind in range(num_sample):\n",
        "        axarr[ind].imshow(x_clean[ind,0,:,:].cpu().numpy(),'gray')\n",
        "    plt.show()\n",
        "    data = {\"img\":img}\n",
        "    print(str(time.time()-aa))\n",
        "    scipy.io.savemat(path+ 'test_example_epoch'+str(epoch)+'.mat',data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_sample = 4 if class_cond ==False else NUM_CLASSES\n",
        "conditions=torch.arange(NUM_CLASSES).long().to(device)\n",
        "def evaluate_fixed_noise(model,epoch,path,noise):\n",
        "    model.eval()\n",
        "    aa = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_clean = diffusion.p_sample_loop(model,(num_sample, channels, image_size, image_size),clip_denoised=True,model_kwargs={'y':conditions},noise=noise)\n",
        "\n",
        "    grid=torchvision.utils.make_grid(x_clean,nrow=NUM_CLASSES)\n",
        "    torchvision.utils.save_image(grid, path+ 'test_example_epoch'+str(epoch)+'.png', normalize=False, )\n",
        "    print('Generate for the epoch #'+str(epoch)+' result:')"
      ],
      "metadata": {
        "id": "QWiMnTEji9SW"
      },
      "id": "QWiMnTEji9SW",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3fc474b1",
      "metadata": {
        "id": "3fc474b1"
      },
      "source": [
        "# Start the training and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_set2 = ImageFolder(training_dt_dir,transform=transforms.Compose([\n",
        "        torchvision.transforms.Grayscale(),\n",
        "         torchvision.transforms.Resize((image_size, image_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ]))\n",
        "train_dev_sets = training_set2#torch.utils.data.ConcatDataset([training_set1, training_set2])\n"
      ],
      "metadata": {
        "id": "Jvg9Fn_bSJxd"
      },
      "id": "Jvg9Fn_bSJxd",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "311f54bb",
      "metadata": {
        "id": "311f54bb"
      },
      "outputs": [],
      "source": [
        "# Enter your data reader parameters\n",
        "params = {'batch_size': BATCH_SIZE_TRAIN,\n",
        "          'shuffle': True,\n",
        "          'pin_memory': True,\n",
        "          'drop_last': False}\n",
        "train_loader1 = torch.utils.data.DataLoader(train_dev_sets, **params)\n",
        "shape_=(num_sample, channels, image_size, image_size)\n",
        "fixed_noise = torch.randn(*shape_, device=device)\n",
        "# Enter your total number of epoch\n",
        "\n",
        "# Enter the address you save the checkpoint and the evaluation examples\n",
        "path = output_dir\n",
        "\n",
        "PATH = path+'ViTRes1.pt' # Use your own path\n",
        "best_loss = 1\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "train_loss_history, test_loss_history = [], []\n",
        "\n",
        "\n",
        "# Uncomment this when you resume the checkpoint\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/mixed_DDPM_transformer/ViTRes1_26.pt\"),strict=False)\n",
        "\n",
        "train_loss= np.array([])\n",
        "\n",
        "for epoch in range(0, N_EPOCHS):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "\n",
        "    average_loss = train(model, optimizer, train_loader1, train_loss_history)\n",
        "    train_loss=np.append(train_loss,average_loss)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    if epoch % 5 == 0:\n",
        "\n",
        "        evaluate_fixed_noise(model,epoch,path,noise=fixed_noise)\n",
        "        print('Save the latest best model')\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "print('Execution time')\n",
        "np.save(path+'loss.npy',train_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sampling**"
      ],
      "metadata": {
        "id": "MzsLemuAHFMH"
      },
      "id": "MzsLemuAHFMH"
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/tmp/ViTRes1.pt\"))"
      ],
      "metadata": {
        "id": "_2v8jDmzJ-rH"
      },
      "id": "_2v8jDmzJ-rH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/output_dir;mkdir /content/output_dir/0;mkdir /content/output_dir/1;mkdir /content/output_dir/2;mkdir /content/output_dir/3"
      ],
      "metadata": {
        "id": "DVKT65QSXxd8"
      },
      "id": "DVKT65QSXxd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sample = 1\n",
        "epochs=1\n",
        "for epoch in range(epochs):\n",
        "    for label in range(NUM_CLASSES):\n",
        "        model.eval()\n",
        "        conditions=torch.ones(1).long().repeat(num_sample).to(device) * label\n",
        "        images=None\n",
        "        with torch.no_grad():\n",
        "            x_clean = diffusion.p_sample_loop(model,(num_sample, 1, image_size, image_size),clip_denoised=True,model_kwargs={'y':conditions},)\n",
        "        for i,img in enumerate(x_clean):\n",
        "            torchvision.utils.save_image(img, f'/content/output_dir/{label}/{epoch}_{i}.png', normalize=False, )\n",
        "\n",
        "        print(f\"end of labeel: {label}\")\n",
        "\n",
        "    print(f\"end of epoch: {epoch}\")\n"
      ],
      "metadata": {
        "id": "yTI-BBdBKa9V"
      },
      "id": "yTI-BBdBKa9V",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "2d12c7c5",
        "4bb44eb1",
        "ae0b32db",
        "3fc474b1",
        "MzsLemuAHFMH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}